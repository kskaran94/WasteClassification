{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Waste classification",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "lDUyqRJYzxQL",
        "-hIpW4ZSz9IT",
        "24IleLXh0WQS",
        "2fQx3wChDV5I"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kskaran94/WasteClassification/blob/master/Waste_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDUyqRJYzxQL",
        "colab_type": "text"
      },
      "source": [
        "## Libraries Imported"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRnJbyjEbFdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Activation, \\\n",
        "Dense, Dropout, Input, add\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, \\\n",
        " recall_score, confusion_matrix\n",
        "from tensorflow.keras.models import save_model, load_model\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hIpW4ZSz9IT",
        "colab_type": "text"
      },
      "source": [
        "## Download Data from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F0pZg5gbV88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f4c5cf78-f73b-48e2-a155-0b764016e3d8"
      },
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "token = {\"username\":\"kskaran94\",\"key\":\"e845a1f4ce47bb7f34dc6ec9f108f676\"}\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "    \n",
        "! chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle datasets download -d techsash/waste-classification-data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "Downloading waste-classification-data.zip to /content\n",
            " 93% 209M/225M [00:05<00:00, 35.1MB/s]\n",
            "100% 225M/225M [00:05<00:00, 40.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb_vLYJ9sCL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip waste-classification-data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24IleLXh0WQS",
        "colab_type": "text"
      },
      "source": [
        "## Dataset File Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1qIDRGl791v",
        "colab_type": "text"
      },
      "source": [
        "The first step after downloading the data would be to look at the data set file structure. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNwVsNcL0Vkx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82993e29-0c35-4b13-ba07-aa7382ab6254"
      },
      "source": [
        "!ls DATASET/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST  TRAIN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViWSSJ8g8Ge9",
        "colab_type": "text"
      },
      "source": [
        "We, see that the dataset has pre-defined train and test splits. We are missing a validation and before any modling or configuration, the task would be to construct a validation set from the existing train test. At this point, test set is untouched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfPZBLgj0KLR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fff2dc73-627a-4e42-ee02-e0164b9396e6"
      },
      "source": [
        "!ls DATASET/TRAIN/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O  R\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDZpL6ai8onH",
        "colab_type": "text"
      },
      "source": [
        "## Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROzcrbc88uK7",
        "colab_type": "text"
      },
      "source": [
        "Build a image classifier to correctly identify Recyclable and Organic waste.  This would help government authorities reduce toxic waste in landfills. Thereby reducing land pollution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlIAeJSq9v3S",
        "colab_type": "text"
      },
      "source": [
        "## Util Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlZ1FWfX9gw2",
        "colab_type": "text"
      },
      "source": [
        "All the custom functions used in the notebook can be found in this block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fQx3wChDV5I",
        "colab_type": "text"
      },
      "source": [
        "### Copying files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaYzd0d6C8bd",
        "colab_type": "text"
      },
      "source": [
        "Function copies given file_nams from source to destination using shutil"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCm22yNPC15Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copyfiles(file_names, dest, src_path):\n",
        "    for file in file_names:\n",
        "        full_file_name = os.path.join(src_path, file)\n",
        "        if os.path.isfile(full_file_name):\n",
        "            shutil.copy(full_file_name, dest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTLQatQgDaG5",
        "colab_type": "text"
      },
      "source": [
        "### Train Validation Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KdzyfEyDiy_",
        "colab_type": "text"
      },
      "source": [
        "Function takes a path of a directory and percentage of train to split the data into train and validation. Sklearn's train_test_split works only with dataframes /arrays. This function is written for  directory-file structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv2s0zKo0ihd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_val_test_split(path, perc):\n",
        "    train_string  = 'train/'\n",
        "    val_string  = 'val/' \n",
        "    dest_path = '/content/'\n",
        "    try:\n",
        "        os.mkdir(dest_path + train_string)\n",
        "        os.mkdir(dest_path + val_string)\n",
        "    except:\n",
        "        shutil.rmtree(dest_path + train_string)\n",
        "        shutil.rmtree(dest_path + val_string)\n",
        "        os.mkdir(dest_path + train_string)\n",
        "        os.mkdir(dest_path + val_string)\n",
        "    \n",
        "    sub_direc = os.listdir(path=path)\n",
        "    \n",
        "    for sub in sub_direc:\n",
        "      if sub in ['O','R']:\n",
        "        try:\n",
        "            shutil.rmtree(dest_path + train_string + sub)\n",
        "            shutil.rmtree(dest_path + val_string + sub)\n",
        "        except:\n",
        "            os.makedirs(dest_path+train_string+sub)\n",
        "            os.makedirs(dest_path+val_string+sub)\n",
        "        src_path = path + sub\n",
        "        filenames = os.listdir(src_path)\n",
        "        filenames.sort()  \n",
        "        # make sure that the filenames have a fixed order before shuffling\n",
        "        random.shuffle(filenames) \n",
        "        # shuffles the ordering of filenames (deterministic given the chosen seed)\n",
        "\n",
        "        split_1 = int(perc * len(filenames))\n",
        "        train_filenames = filenames[:split_1]\n",
        "        val_filenames = filenames[split_1:]\n",
        "\n",
        "        copyfiles(train_filenames, dest_path+train_string+sub, src_path)\n",
        "        ## train set path for all classes\n",
        "        copyfiles(val_filenames, dest_path+val_string+sub, src_path)\n",
        "        ## validation set path for all classes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQm0ArzuGyj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_val_test_split('DATASET/TRAIN/', 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZubJ2DchHcoE",
        "colab_type": "text"
      },
      "source": [
        "### Predict from generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjQe1dSYHafn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_from_generator(generator, model): \n",
        "    pred = model.predict_generator(generator)\n",
        "    predicted_class_indices = np.argmax(pred, axis = -1)\n",
        "    classes = generator.classes[generator.index_array]\n",
        "    return predicted_class_indices, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fPzxLgNEHlc",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation and Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-ZZ-3zOEgeD",
        "colab_type": "text"
      },
      "source": [
        "Preparing the data for the model is an important task. In case of images, standard preparation techniques include rescaling, resizing and data augmentation (if needed). Keras provides ImageDataGenerator class for data preparation. We will be defining three different ImageDataGenerators for train, validation and test sets respectively. \n",
        "\n",
        "Rescaling of images is defined within the ImageDataGenerator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2magZPxEDGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=64\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmK0q5B3FRJo",
        "colab_type": "text"
      },
      "source": [
        "In this case batch size is a paramter which can be tuned and the evaluation metric may also vary with different batch_size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tybTMdylF2Cw",
        "colab_type": "text"
      },
      "source": [
        "The Image data generator is used for returning configured images using the flow functions. We will be using the flow_from_directory to configure the images. There are other ways which can found here https://keras.io/preprocessing/image/\n",
        "\n",
        "In this code block, the paramter that can be tuned is the target size of the image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhA_KOMvFQey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "            'train/',  # this is the target directory\n",
        "            target_size=(150, 150),  # all images will be resized to 150x150\n",
        "            batch_size=batch_size,\n",
        "            class_mode='categorical',shuffle=False)\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "        'val/',  # this is the target directory\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',shuffle=False)\n",
        "\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        'DATASET/TEST/',  # this is the target directory\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWw5l9nSG5dT",
        "colab_type": "text"
      },
      "source": [
        "## Model Defintion and Compilation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRh03Xng41Kx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3067f0f8-d71e-42d6-be67-e71f0bc6b4dd"
      },
      "source": [
        "num_classes = 2\n",
        "input_shape = (150, 150, 3)\n",
        "\n",
        "cnn_small_bn = Sequential([\n",
        "    Conv2D(8, kernel_size = (3,3), input_shape=input_shape, activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(8, kernel_size = (3,3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(num_classes, activation='softmax'),\n",
        "                 ])\n",
        "\n",
        "cnn_small_bn.summary()\n",
        "\n",
        "cnn_small_bn.compile(\"adam\", \"categorical_crossentropy\",\n",
        "                     metrics=['accuracy'])\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 148, 148, 8)       224       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 74, 74, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 72, 72, 8)         584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 36, 36, 8)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10368)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 20738     \n",
            "=================================================================\n",
            "Total params: 21,546\n",
            "Trainable params: 21,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USy-U69ZHlkN",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5axwSbQE5GzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "history_cnn = cnn_small_bn.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=20,\n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=20)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hUGgto_Hrkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(history_cnn.history).plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzpDfCj87A8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ef0cbd5c-1728-45f6-a1bc-2964faf0a901"
      },
      "source": [
        "val_pred, val_classes = predict_from_generator(val_generator, cnn_small_bn)\n",
        "\n",
        "\n",
        "confusion_matrix(val_classes, val_pred)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1666,  848],\n",
              "       [ 483, 1516]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5I4EEL_5abQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# avg_conf_matrix_val = np.zeros((num_classes, num_classes))\n",
        "\n",
        "# for i in range(num_fold):\n",
        "#     avg_conf_matrix_val += confusion_matrix(val_class_arr, val_pred_arr[i])\n",
        "    \n",
        "# avg_conf_matrix_val  /= num_fold\n",
        "\n",
        "# avg_conf_matrix_val = avg_conf_matrix_val.astype('int')\n",
        "\n",
        "# fig5 = sns.heatmap(avg_conf_matrix_val, annot=True, fmt=\"d\")\n",
        "\n",
        "# _ = fig5.set_xticklabels(class_names)\n",
        "\n",
        "# fig1 = sns.barplot(class_names, calc_specificity(avg_conf_matrix_val))\n",
        "\n",
        "# _ = fig1.set_title(\"Specificity values over all classes for validation set\")\n",
        "\n",
        "# _ = fig1.set(xlabel = \"Class names\", ylabel = \"Specificity value\")\n",
        "\n",
        "# _ = fig1.set_ylim(0,1.2)\n",
        "\n",
        "# for index, val in enumerate(calc_specificity(avg_conf_matrix_val)):\n",
        "#     fig1.text(index,round(val,3) + 0.02 ,round(val,3), color='black', size = 12)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}