{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Waste classification",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kskaran94/WasteClassification/blob/master/Waste_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDUyqRJYzxQL",
        "colab_type": "text"
      },
      "source": [
        "## Libraries Imported"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRnJbyjEbFdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Activation, \\\n",
        "Dense, Dropout, Input, add\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, \\\n",
        " recall_score, confusion_matrix\n",
        "from tensorflow.keras.models import save_model, load_model\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hIpW4ZSz9IT",
        "colab_type": "text"
      },
      "source": [
        "## Download Data from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F0pZg5gbV88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f4c5cf78-f73b-48e2-a155-0b764016e3d8"
      },
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "token = {\"username\":\"kskaran94\",\"key\":\"e845a1f4ce47bb7f34dc6ec9f108f676\"}\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "    \n",
        "! chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle datasets download -d techsash/waste-classification-data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "Downloading waste-classification-data.zip to /content\n",
            " 93% 209M/225M [00:05<00:00, 35.1MB/s]\n",
            "100% 225M/225M [00:05<00:00, 40.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb_vLYJ9sCL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip waste-classification-data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24IleLXh0WQS",
        "colab_type": "text"
      },
      "source": [
        "## Dataset File Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1qIDRGl791v",
        "colab_type": "text"
      },
      "source": [
        "The first step after downloading the data would be to look at the data set file structure. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNwVsNcL0Vkx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82993e29-0c35-4b13-ba07-aa7382ab6254"
      },
      "source": [
        "!ls DATASET/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST  TRAIN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViWSSJ8g8Ge9",
        "colab_type": "text"
      },
      "source": [
        "We, see that the dataset has pre-defined train and test splits. We are missing a validation and before any modling or configuration, the task would be to construct a validation set from the existing train test. At this point, test set is untouched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfPZBLgj0KLR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fff2dc73-627a-4e42-ee02-e0164b9396e6"
      },
      "source": [
        "!ls DATASET/TRAIN/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O  R\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDZpL6ai8onH",
        "colab_type": "text"
      },
      "source": [
        "## Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROzcrbc88uK7",
        "colab_type": "text"
      },
      "source": [
        "Build a image classifier to correctly identify Recyclable and Organic waste.  This would help government authorities reduce toxic waste in landfills. Thereby reducing land pollution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlIAeJSq9v3S",
        "colab_type": "text"
      },
      "source": [
        "## Util Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlZ1FWfX9gw2",
        "colab_type": "text"
      },
      "source": [
        "All the custom functions used in the notebook can be found in this block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4np56IF9dch",
        "colab_type": "text"
      },
      "source": [
        "## Util Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv2s0zKo0ihd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def copyfiles(file_names, dest, src_path):\n",
        "    for file in file_names:\n",
        "        full_file_name = os.path.join(src_path, file)\n",
        "        if os.path.isfile(full_file_name):\n",
        "            shutil.copy(full_file_name, dest)\n",
        "            \n",
        "\n",
        "\n",
        "def train_val_test_split(path, perc):\n",
        "    train_string  = 'train/'\n",
        "    val_string  = 'val/' \n",
        "    dest_path = '/content/'\n",
        "    try:\n",
        "        os.mkdir(dest_path + train_string)\n",
        "        os.mkdir(dest_path + val_string)\n",
        "    except:\n",
        "        shutil.rmtree(dest_path + train_string)\n",
        "        shutil.rmtree(dest_path + val_string)\n",
        "        os.mkdir(dest_path + train_string)\n",
        "        os.mkdir(dest_path + val_string)\n",
        "    \n",
        "    sub_direc = os.listdir(path=path)\n",
        "    \n",
        "    for sub in sub_direc:\n",
        "      if sub in ['O','R']:\n",
        "        try:\n",
        "            shutil.rmtree(dest_path + train_string + sub)\n",
        "            shutil.rmtree(dest_path + val_string + sub)\n",
        "        except:\n",
        "            os.makedirs(dest_path+train_string+sub)\n",
        "            os.makedirs(dest_path+val_string+sub)\n",
        "        src_path = path + sub\n",
        "        filenames = os.listdir(src_path)\n",
        "        filenames.sort()  # make sure that the filenames have a fixed order before shuffling\n",
        "        random.shuffle(filenames) # shuffles the ordering of filenames (deterministic given the chosen seed)\n",
        "\n",
        "        split_1 = int(perc * len(filenames))\n",
        "        train_filenames = filenames[:split_1]\n",
        "        val_filenames = filenames[split_1:]\n",
        "\n",
        "        copyfiles(train_filenames, dest_path+train_string+sub, src_path) ## train set path for all classes\n",
        "        copyfiles(val_filenames, dest_path+val_string+sub, src_path) ## validation set path for all classes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixeHky3x8t30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ-vj5m13Wc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_val_test_split('DATASET/TRAIN/', 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWK7qPOp1oAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=64\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRh03Xng41Kx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "69771349-10c8-44e1-bc2a-993a1806e774"
      },
      "source": [
        "num_classes = 2\n",
        "input_shape = (150, 150, 3)\n",
        "\n",
        "cnn_small_bn = Sequential([\n",
        "    Conv2D(8, kernel_size = (3,3), input_shape=input_shape, activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(8, kernel_size = (3,3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(num_classes, activation='softmax'),\n",
        "                 ])\n",
        "\n",
        "cnn_small_bn.summary()\n",
        "\n",
        "cnn_small_bn.compile(\"adam\", \"categorical_crossentropy\",\n",
        "                     metrics=['accuracy'])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0715 18:35:26.693153 140644382537600 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 148, 148, 8)       224       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 74, 74, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 72, 72, 8)         584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 8)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10368)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 20738     \n",
            "=================================================================\n",
            "Total params: 21,546\n",
            "Trainable params: 21,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy-DwkHJ5Gfy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5axwSbQE5GzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_from_generator(generator, model): \n",
        "    pred = model.predict_generator(generator)\n",
        "    predicted_class_indices = np.argmax(pred, axis = -1)\n",
        "    classes = generator.classes[generator.index_array]\n",
        "    return predicted_class_indices, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wltu7Lgd45w7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "754d58b9-179a-4762-9932-a16d4e179766"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "            'train/',  # this is the target directory\n",
        "            target_size=(150, 150),  # all images will be resized to 150x150\n",
        "            batch_size=batch_size,\n",
        "            class_mode='categorical',shuffle=False)\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "        'val/',  # this is the target directory\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',shuffle=False)\n",
        "\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        'DATASET/TEST/',  # this is the target directory\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',shuffle=False)\n",
        "\n",
        "history_cnn = cnn_small_bn.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=20,\n",
        "    epochs=20,\n",
        "    verbose=1,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=20)\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 18051 images belonging to 2 classes.\n",
            "Found 4513 images belonging to 2 classes.\n",
            "Found 2513 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "20/20 [==============================] - 7s 348ms/step - loss: 0.7016 - acc: 0.6187 - val_loss: 0.4525 - val_acc: 0.8992\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 4s 182ms/step - loss: 0.5967 - acc: 0.6852 - val_loss: 0.2203 - val_acc: 1.0000\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 4s 182ms/step - loss: 0.6242 - acc: 0.6367 - val_loss: 0.5845 - val_acc: 0.6703\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 4s 182ms/step - loss: 0.6135 - acc: 0.7250 - val_loss: 0.3322 - val_acc: 0.8891\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 4s 182ms/step - loss: 0.6440 - acc: 0.6539 - val_loss: 0.3991 - val_acc: 0.8375\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 4s 182ms/step - loss: 0.6629 - acc: 0.5844 - val_loss: 0.3886 - val_acc: 0.9594\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 4s 183ms/step - loss: 0.6388 - acc: 0.6445 - val_loss: 0.4034 - val_acc: 0.9078\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 4s 182ms/step - loss: 0.6236 - acc: 0.7297 - val_loss: 0.4190 - val_acc: 0.8266\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 4s 179ms/step - loss: 0.5712 - acc: 0.7570 - val_loss: 0.2513 - val_acc: 0.9125\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 4s 182ms/step - loss: 0.6139 - acc: 0.7359 - val_loss: 2.0517 - val_acc: 0.0250\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 4s 177ms/step - loss: 0.7959 - acc: 0.6284 - val_loss: 0.2481 - val_acc: 0.9969\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 4s 180ms/step - loss: 0.6790 - acc: 0.5883 - val_loss: 0.3545 - val_acc: 0.9906\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 4s 184ms/step - loss: 0.5955 - acc: 0.6914 - val_loss: 0.3624 - val_acc: 0.9781\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 3s 158ms/step - loss: 0.6306 - acc: 0.6711 - val_loss: 0.3817 - val_acc: 0.9492\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 4s 216ms/step - loss: 0.6068 - acc: 0.7016 - val_loss: 0.2506 - val_acc: 0.9867\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 4s 183ms/step - loss: 0.4459 - acc: 0.8258 - val_loss: 0.3258 - val_acc: 0.9141\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 4s 177ms/step - loss: 2.8537 - acc: 0.6812 - val_loss: 0.6025 - val_acc: 0.7430\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 4s 178ms/step - loss: 0.5994 - acc: 0.6367 - val_loss: 0.2383 - val_acc: 0.9953\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 3s 173ms/step - loss: 0.5438 - acc: 0.7156 - val_loss: 0.1639 - val_acc: 0.9906\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 4s 179ms/step - loss: 0.8027 - acc: 0.4758 - val_loss: 0.6058 - val_acc: 0.7141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzpDfCj87A8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ef0cbd5c-1728-45f6-a1bc-2964faf0a901"
      },
      "source": [
        "val_pred, val_classes = predict_from_generator(val_generator, cnn_small_bn)\n",
        "\n",
        "\n",
        "confusion_matrix(val_classes, val_pred)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1666,  848],\n",
              "       [ 483, 1516]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5I4EEL_5abQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# avg_conf_matrix_val = np.zeros((num_classes, num_classes))\n",
        "\n",
        "# for i in range(num_fold):\n",
        "#     avg_conf_matrix_val += confusion_matrix(val_class_arr, val_pred_arr[i])\n",
        "    \n",
        "# avg_conf_matrix_val  /= num_fold\n",
        "\n",
        "# avg_conf_matrix_val = avg_conf_matrix_val.astype('int')\n",
        "\n",
        "# fig5 = sns.heatmap(avg_conf_matrix_val, annot=True, fmt=\"d\")\n",
        "\n",
        "# _ = fig5.set_xticklabels(class_names)\n",
        "\n",
        "# fig1 = sns.barplot(class_names, calc_specificity(avg_conf_matrix_val))\n",
        "\n",
        "# _ = fig1.set_title(\"Specificity values over all classes for validation set\")\n",
        "\n",
        "# _ = fig1.set(xlabel = \"Class names\", ylabel = \"Specificity value\")\n",
        "\n",
        "# _ = fig1.set_ylim(0,1.2)\n",
        "\n",
        "# for index, val in enumerate(calc_specificity(avg_conf_matrix_val)):\n",
        "#     fig1.text(index,round(val,3) + 0.02 ,round(val,3), color='black', size = 12)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}